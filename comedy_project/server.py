import uvicorn
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse, FileResponse
import numpy as np
import soundfile as sf
import io
import asyncio
import random
from collections import deque
import subprocess
import time
from openai import AsyncOpenAI
import base64
import re

app = FastAPI()

#vLLM connecting information
client = AsyncOpenAI(
    api_key="comedy_key",
    base_url="http://localhost:8000/v1"
)

MODEL_NAME = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
MIN_VOLUME_THRESHOLD = 0.05

# === Tuning knobs (MVP defaults) ===
# [laugh] ë§ˆì»¤ ë’¤ì— ê´€ê° ë°˜ì‘(ì›ƒìŒ)ì„ ê¸°ë‹¤ë¦¬ëŠ” ì‹œê°„(ì´ˆ)
SILENCE_DURATION = 1.2

# ë¸Œë¼ìš°ì €(MediaRecorder)ì—ì„œ ë³´ë‚´ëŠ” ì˜¤ë””ì˜¤ ì„¤ì •ê³¼ "ë°˜ë“œì‹œ" ë§ì¶°ì•¼ í•¨
TARGET_SAMPLE_RATE = 16000
TARGET_CHANNELS = 1
MEDIARECORDER_TIMESLICE_MS = 500  # frontend.htmlì˜ mediaRecorder.start(500) ê³¼ ë™ì¼í•´ì•¼ í•¨

# AI ë¶„ë¥˜ì— ë„£ì„ ì˜¤ë””ì˜¤ ìœˆë„ìš° ê¸¸ì´(ì´ˆ)
AI_WINDOW_SEC = 1.5

# WebM ì¡°ê°ì„ ë„ˆë¬´ ë§ì´ ìŒ“ì§€ ì•Šê¸° ìœ„í•œ ì œí•œ(500ms * 20 = 10ì´ˆ ì •ë„)
MAX_WEBM_SEGMENTS = 20

# ì—°ì†ëœ í° ì†Œë¦¬ì— ëŒ€í•´ ë„ˆë¬´ ìì£¼ íŠ¸ë¦¬ê±° ë˜ì§€ ì•Šë„ë¡ ì¿¨ë‹¤ìš´(ì´ˆ)
TRIGGER_COOLDOWN_SEC = 1.0

# ìƒíƒœ ê´€ë¦¬
class ComedyState:
    def __init__(self):
        self.script_queue = deque()
        self.is_speaking = False
        self.expecting_laugh = False
        self.current_mood = "normal"
        self.interrupted = False

state = ComedyState()

#ì›¹í˜ì´ì§€ ì ‘ì† ì‹œ index.html ì „ì†¡
@app.get("/")
async def get():
    return FileResponse("frontend.html")


def decode_webm_to_pcm16(webm_bytes: bytes, *, sr: int = TARGET_SAMPLE_RATE, ch: int = TARGET_CHANNELS) -> bytes:
    """MediaRecorder(webm/opus)ë¡œ ë°›ì€ ë°”ì´íŠ¸ë¥¼ PCM16(raw s16le)ë¡œ ë””ì½”ë”©.

    - ffmpegê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨.
    - ë°˜í™˜ê°’: little-endian 16-bit PCM ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼ (í—¤ë” ì—†ìŒ)
    """
    cmd = [
        "ffmpeg",
        "-hide_banner",
        "-loglevel",
        "error",
        "-i",
        "pipe:0",
        "-ac",
        str(ch),
        "-ar",
        str(sr),
        "-f",
        "s16le",
        "pipe:1",
    ]

    try:
        p = subprocess.run(cmd, input=webm_bytes, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=False)
    except FileNotFoundError as e:
        raise RuntimeError("ffmpegê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. (apt-get install ffmpeg)") from e

    if p.returncode != 0 or not p.stdout:
        err = (p.stderr or b"").decode("utf-8", errors="ignore")
        raise RuntimeError(f"ffmpeg ë””ì½”ë”© ì‹¤íŒ¨: {err[:400]}")

    return p.stdout


def pcm16_to_wav_bytes(pcm16_bytes: bytes, *, sr: int = TARGET_SAMPLE_RATE, ch: int = TARGET_CHANNELS) -> bytes:
    """PCM16(raw) -> WAV íŒŒì¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜ (ëª¨ë¸ ì…ë ¥ìš©)."""
    audio_i16 = np.frombuffer(pcm16_bytes, dtype=np.int16)
    audio_f32 = audio_i16.astype(np.float32) / 32768.0

    if ch > 1:
        audio_f32 = audio_f32.reshape(-1, ch)

    buf = io.BytesIO()
    sf.write(buf, audio_f32, sr, format="WAV", subtype="PCM_16")
    return buf.getvalue()


def rms_volume_from_pcm16(pcm16_bytes: bytes) -> float:
    """PCM16(raw)ì—ì„œ RMS ë³¼ë¥¨ ê³„ì‚°."""
    if not pcm16_bytes:
        return 0.0
    audio_i16 = np.frombuffer(pcm16_bytes, dtype=np.int16).astype(np.float32)
    audio_f32 = audio_i16 / 32768.0
    return float(np.sqrt(np.mean(audio_f32 ** 2)))

async def script_producer():
    # ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ì˜ˆì‹œ ëŒ€ë³¸
    scripts_db = {
        "normal": [
            "You know, AI dating is hard. I matched with a toaster yesterday. [laugh]",
            "Why did the robot cross the road? To optimize the pathfinding algorithm! [laugh]",
        ],
        "awkward": [ # ë°˜ì‘ ì•ˆ ì¢‹ì„ ë•Œ ìˆ˜ìŠµìš©
            "Wow, tough crowd today. Is my microphone on? [laugh]",
            "Okay, okay, I get it. Not fans of tech jokes. Let's talk about humans. [laugh]",
            "This silence is louder than my cooling fan. [laugh]"
        ],
        "hyped": [ # ë°˜ì‘ ì¢‹ì„ ë•Œ ë” ë‹¬ë¦¬ëŠ”ìš©
            "You guys are on fire! I love this energy! [laugh]",
            "Since you liked that, let me tell you about my GPU's dating life... [laugh]"
        ]
    }

    while True:
        # íê°€ ë„ˆë¬´ ë§ì´ ìŒ“ì´ì§€ ì•Šê²Œ ê´€ë¦¬
        if len(state.script_queue) < 3:
            new_line = random.choice(scripts_db.get(state.current_mood, scripts_db["normal"]))
            
            state.script_queue.append(new_line)
            print(f"ğŸ“ Script Generated ({state.current_mood}): {new_line}")
        
        await asyncio.sleep(2) # 2ì´ˆë§ˆë‹¤ ì²´í¬

async def talker_task(websocket: WebSocket):
    try:
        while True:
            if state.interrupted:
                await asyncio.sleep(0.1)
                continue

            if state.script_queue:
                line = state.script_queue.popleft()
                
                has_laugh_marker = "[laugh]" in line
                clean_line = line.replace("[laugh]", "").strip()

                state.is_speaking = True
                await websocket.send_text(f"comedian: {clean_line}")
                
                await asyncio.sleep(len(clean_line) * 0.06) 
                state.is_speaking = False

                if has_laugh_marker:
                    print("Waiting for laugh...")
                    state.expecting_laugh = True
                    await asyncio.sleep(SILENCE_DURATION) # ê´€ê° ë°˜ì‘ ê¸°ë‹¤ë¦¼

                    state.expecting_laugh = False
                    
                    print(f"Mood updated to: {state.current_mood}")

            else:
                await asyncio.sleep(0.5)
            
    except Exception as e:
        print(f"Talker Error: {e}")

LABELS = {"laughter", "heckle", "noise"}

async def classify_sound(audio_bytes: bytes) -> str:

    audio_b64 = base64.b64encode(audio_bytes).decode("utf-8")

    resp = await client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": (
                            "Listen to this audio and classify it as ONLY one word:\n"
                            "- laughter\n"
                            "- heckle (speech/shouting)\n"
                            "- noise\n"
                            "Reply with ONLY the word, no punctuation.\n"
                            "If there is any intelligible human speech/shouting, choose heckle.\n"
                            "Otherwise choose laughter if it sounds like laughing; else noise."
                        ),
                    },
                    {
                        "type": "audio_url",
                        "audio_url": {"url": f"data:audio/wav;base64,{audio_b64}"},
                    },
                ],
            }
        ],
        modalities=["text"],
        temperature=0,      
        max_tokens=5,
    )

    raw = (resp.choices[0].message.content or "").strip().lower()
    token = re.sub(r"[^a-z]", "", raw)

    if token not in LABELS:
        return "noise"
    return token

async def listener_task(websocket: WebSocket):
    print("Listener Activated")

    # MediaRecorderê°€ ë³´ë‚´ëŠ” ê²ƒì€ PCMì´ ì•„ë‹ˆë¼ webm(opus) ì¡°ê°ì„.
    # => ffmpegë¡œ ë””ì½”ë”©í•´ì„œ PCMìœ¼ë¡œ ë§Œë“  ë’¤ ë³¼ë¥¨/RMS ê³„ì‚° & ëª¨ë¸ ë¶„ë¥˜ ì…ë ¥(WAV)ìœ¼ë¡œ ì‚¬ìš©.

    init_chunk = None  # ì²« ì¡°ê°(í—¤ë”+ì´ˆê¸° ë°ì´í„°)
    segments = deque(maxlen=MAX_WEBM_SEGMENTS)  # ì´í›„ ì¡°ê°ë“¤

    # ì²« ì¡°ê°ì— í¬í•¨ëœ ì´ˆê¸° ì˜¤ë””ì˜¤ê°€ ë§¤ë²ˆ ì¬í¬í•¨ë˜ëŠ” ê²ƒì„ ì¤„ì´ê¸° ìœ„í•œ "ëŒ€ëµ" ë“œë¡­ ë°”ì´íŠ¸
    init_drop_bytes = int((MEDIARECORDER_TIMESLICE_MS / 1000.0) * TARGET_SAMPLE_RATE * 2 * TARGET_CHANNELS)
    ai_window_bytes = int(AI_WINDOW_SEC * TARGET_SAMPLE_RATE * 2 * TARGET_CHANNELS)

    # AI_WINDOW_SECì— í•„ìš”í•œ ì¡°ê° ê°œìˆ˜(ëŒ€ëµ)
    needed_segments = max(1, int(np.ceil((AI_WINDOW_SEC * 1000.0) / MEDIARECORDER_TIMESLICE_MS)))

    last_trigger_ts = 0.0

    try:
        while True:
            # 1) ì˜¤ë””ì˜¤(webm) ì¡°ê° ìˆ˜ì‹ 
            chunk = await websocket.receive_bytes()

            if init_chunk is None:
                init_chunk = chunk
                continue

            segments.append(chunk)

            # 2) ì¶©ë¶„íˆ ëª¨ì´ê¸° ì „ì—” ìŠ¤í‚µ
            if len(segments) < needed_segments:
                continue

            # 3) ì¿¨ë‹¤ìš´
            now = time.time()
            if now - last_trigger_ts < TRIGGER_COOLDOWN_SEC:
                continue

            # 4) webm ì¡°ê°ë“¤ì„ í•©ì³ì„œ ffmpeg ë””ì½”ë”©
            webm_blob = init_chunk + b"".join(list(segments)[-needed_segments:])

            try:
                pcm16 = decode_webm_to_pcm16(webm_blob)
            except Exception as e:
                print(f"FFmpeg Decode Error: {e}")
                continue

            # init_chunkì— ìˆëŠ” ì´ˆê¸° ì˜¤ë””ì˜¤ê°€ ì„ì´ì§€ ì•Šë„ë¡ ì•ë¶€ë¶„ì„ ëŒ€ëµ ì œê±°
            pcm16_eff = pcm16[init_drop_bytes:] if len(pcm16) > init_drop_bytes else pcm16

            # ë§ˆì§€ë§‰ AI_WINDOW_SEC ë§Œí¼ë§Œ ì‚¬ìš©
            pcm16_window = pcm16_eff[-ai_window_bytes:] if len(pcm16_eff) > ai_window_bytes else pcm16_eff

            # 5) ë³¼ë¥¨ ì²´í¬ (1ì°¨ í•„í„°)
            volume = rms_volume_from_pcm16(pcm16_window)
            if volume < MIN_VOLUME_THRESHOLD:
                continue

            print(f"Sound detected (Vol: {volume:.3f}). Asking AI...")

            # 6) ëª¨ë¸ ë¶„ë¥˜ ì…ë ¥ìš© WAVë¡œ ì¸ì½”ë”©
            try:
                wav_bytes = pcm16_to_wav_bytes(pcm16_window)
            except Exception as e:
                print(f"WAV Encode Error: {e}")
                continue

            # 7) AIì—ê²Œ íŒë³„ ìš”ì²­ (2ì°¨ í•„í„°)
            sound_type = await classify_sound(wav_bytes)
            last_trigger_ts = now

            # í•œ ë²ˆ í¬ê²Œ ë°˜ì‘ì´ ì¡í˜”ìœ¼ë©´, ê°™ì€ ë°˜ì‘ì´ ê³„ì† ì´ì–´ì§ˆ ë•Œ ì¤‘ë³µ íŒë³„ì´ ì¦ì§€ ì•Šë„ë¡ ë¹„ì›€
            segments.clear()

            # === íŒë‹¨ì— ë”°ë¥¸ í–‰ë™ ===
            
            # CASE A: ì›ƒìŒ (Laughter)
            if "laugh" in sound_type:
                if state.expecting_laugh:
                    state.current_mood = "hyped"
                else:
                    state.current_mood = "hyped"

            # CASE B: ë¼ì–´ë“¤ê¸°/ì•¼ìœ  (Heckle)
            elif "heckle" in sound_type or "speech" in sound_type or "shout" in sound_type:
                # ë°°ìš°ê°€ ë§í•˜ëŠ” ì¤‘ì¼ ë•Œë§Œ ë¼ì–´ë“¤ê¸°ë¡œ ì¸ì • (í˜¹ì€ í•­ìƒ ì¸ì •)
                if not state.is_speaking: 
                    print("Heckler Detected!")
                    state.interrupted = True
                    
                    # ë°˜ê²© ë©˜íŠ¸ ìƒì„± (ë‚˜ì¤‘ì—” ì—¬ê¸°ë„ AI ìƒì„±ìœ¼ë¡œ êµì²´)
                    heckle_response = "Oh, you have an opinion? That's cute. [laugh]"
                    
                    # í ë§¨ ì•ì— ê¸´ê¸‰ íˆ¬ì… (ìƒˆì¹˜ê¸°)
                    state.script_queue.appendleft(heckle_response)
                    
                    # ìƒíƒœ ë³µêµ¬
                    state.interrupted = False
                    state.current_mood = "hyped"
            
            # CASE C: ì†ŒìŒ (Noise)
            else:
                print("Ignore (Noise)")

    except WebSocketDisconnect:
        print("Listener Stopped")
    except Exception as e:
        print(f"Listener Error: {e}")

#ì‹¤ì‹œê°„ í†µì‹ 
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    print("Client connected")
    
    # 3ê°œì˜ íƒœìŠ¤í¬ ë™ì‹œ ì‹¤í–‰
    producer = asyncio.create_task(script_producer())
    talker = asyncio.create_task(talker_task(websocket))
    listener = asyncio.create_task(listener_task(websocket))

    try:
        # ë©”ì¸ ë£¨í”„ëŠ” íƒœìŠ¤í¬ë“¤ì´ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
        await asyncio.gather(producer, talker, listener)
    except Exception as e:
        print(f"Main Error: {e}")
    finally:
        producer.cancel()
        talker.cancel()
        listener.cancel()

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8080)