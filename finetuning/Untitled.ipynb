{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f71df5-d9a9-4f84-bc6a-51fe48ac7d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip -q install -U \"transformers>=4.41\" datasets accelerate peft bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d07b1e-b5fb-4e3a-8e92-6513a7c0f73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script json count: 31\n",
      "sample: [PosixPath('/workspace/standup_scripts/json/.ipynb_checkpoints/script__fJ3pO5CFxg-checkpoint.json'), PosixPath('/workspace/standup_scripts/json/script_1e9GH83u_Gc.json'), PosixPath('/workspace/standup_scripts/json/script_2KCnrU0S7Vw.json')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path(\"/workspace/standup_scripts\")   # 너가 python으로 extract한 폴더\n",
    "json_files = sorted(OUT_DIR.rglob(\"script_*.json\"))\n",
    "print(\"script json count:\", len(json_files))\n",
    "print(\"sample:\", json_files[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2115c95a-502d-4b88-a6ec-7562c17aae4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf415fcf4744cb396d361c058cb94de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae925edd03ad4d2688878fa3ece88cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043eb0b308c34a05a44661aef9846def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1bafd6cbeb49eba98a8096817a9a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d1329c82214c9d988e46946728b9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c22fe43ed794a89ad1ad6948818bbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81c4ff5399c42f29a8b817dd84cc038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05436b95c48b43b4a4c2a1a2c806f7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6eced0e84948509311e0972a478ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc7515a53914fcbbb6cd93820b37d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44dc53eb33e941488c96d56e9305d1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2791dfe104cc47e19c2cddade3b1a780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7726496c42bd4c3b880a06355058f817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\" \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # ✅ 4bit 양자화(메모리 크게 절약)\n",
    "    bnb_4bit_quant_type=\"nf4\",            # QLoRA에서 자주 쓰는 방식\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 메모리 절약 옵션(긴 시퀀스에 특히 도움)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(\"loaded:\", BASE_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dbcee38-18d4-4d7f-9482-5e7067c13478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build samples: 100%|██████████| 31/31 [00:02<00:00, 13.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples: 337\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 326\n",
      "}) Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 11\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def timeline_to_text(obj):\n",
    "    parts = []\n",
    "    for ev in obj.get(\"timeline\", []):\n",
    "        c = (ev.get(\"content\") or \"\").strip()\n",
    "        if c:\n",
    "            parts.append(c)\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a stand-up comedian writing a live performance script.\\n\"\n",
    "    \"Keep tags exactly as-is when present (e.g., [laughter], [applause], [SETUP#], [CALLBACK#]).\\n\"\n",
    "    \"Continue naturally with good pacing and callbacks.\"\n",
    ")\n",
    "\n",
    "PROMPT_LEN = 2048\n",
    "ANSWER_LEN = 1024\n",
    "STRIDE = 1024\n",
    "\n",
    "samples = []\n",
    "for fp in tqdm(json_files, desc=\"build samples\"):\n",
    "    obj = json.loads(Path(fp).read_text(encoding=\"utf-8\"))\n",
    "    full_text = timeline_to_text(obj)\n",
    "    if len(full_text) < 2000:\n",
    "        continue\n",
    "\n",
    "    ids = tokenizer.encode(full_text)\n",
    "    total = len(ids)\n",
    "    win = PROMPT_LEN + ANSWER_LEN\n",
    "\n",
    "    for start in range(0, max(0, total - win), STRIDE):\n",
    "        prompt = tokenizer.decode(ids[start:start+PROMPT_LEN])\n",
    "        answer = tokenizer.decode(ids[start+PROMPT_LEN:start+win])\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": \"Continue the following stand-up script in the same style:\\n\\n\" + prompt},\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "\n",
    "        if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\n",
    "            text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        else:\n",
    "            text = (\n",
    "                f\"<|system|>\\n{SYSTEM_PROMPT}\\n\"\n",
    "                f\"<|user|>\\nContinue the following stand-up script in the same style:\\n\\n{prompt}\\n\"\n",
    "                f\"<|assistant|>\\n{answer}\\n\"\n",
    "            )\n",
    "\n",
    "        samples.append({\"text\": text})\n",
    "\n",
    "print(\"total samples:\", len(samples))\n",
    "ds = Dataset.from_list(samples).train_test_split(test_size=0.03, seed=42)\n",
    "train_ds, eval_ds = ds[\"train\"], ds[\"test\"]\n",
    "print(train_ds, eval_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18ab0aa1-6c15-495d-927c-4a3ca62e44b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p /workspace/finetuning/qwen2.5-standup-sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eac3079-fe82-4ac4-b28a-a1d6ed839b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed2c27bf680423eaaaf8223716f88a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7da9c2cc294bd6a6f3730317600242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6bc6494f3774fa0b484a45f9032ddd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e821df2e23041b3afc845f2a98c3435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa21a9fc206452b88debc5cbe1a4d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ece815053aa48bd80c0b7b216ec2488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 17:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.305400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved adapter to qwen25-standup-sft/adapter\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "lora = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Qwen 계열도 보통 이 프로젝션들이 핵심 학습 지점\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")\n",
    "\n",
    "cfg = SFTConfig(\n",
    "    output_dir=\"/workspace/finetuning/qwen2.5-standup-sft\",\n",
    "    dataset_text_field=\"text\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,    \n",
    "    max_length=4096,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    bf16=True,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=cfg,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    peft_config=lora,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 어댑터 저장(이게 결과물)\n",
    "trainer.save_model(\"/workspace/finetuning/qwen25-standup-sft/adapter\")\n",
    "print(\"saved adapter to qwen25-standup-sft/adapter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "185281d0-41e9-4cb8-a62f-1ea401390bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16fbd34839144cc83ef11d372a40bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a stand-up comedian writing a live performance script.\n",
      "Keep tags exactly as-is when present (e.g., [laughter], [applause], [SETUP#], [CALLBACK#]).\n",
      "Continue naturally with good pacing and callbacks.<|im_end|>\n",
      "<|im_start|>user\n",
      "Write a 15-minute stand-up set about university student. Include 3 callbacks. Use [laughter] tags.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "[laughter]\n",
      "Hi, I'm your dad's favorite son.\n",
      "I got to be careful with what I say.\n",
      "My dad's my favorite person in the world,\n",
      "but my mom's my favorite person.\n",
      "I love him to death,\n",
      "but he's not my favorite.\n",
      "He's got some real things that he's into\n",
      "that I'm gonna tell you about today,\n",
      "so bear with me for a little while.\n",
      "So, when I was a kid,\n",
      "I remember being very interested\n",
      "in my dad's college experience,\n",
      "and I loved asking questions.\n",
      "And every time he'd tell me stories about it,\n",
      "I would just get more and more excited.\n",
      "Because I wanted to go to the same school.\n",
      "But my dad never went to university,\n",
      "so I always had this dream of going there.\n",
      "And one day,\n",
      "my dad came home from work one day.\n",
      "And he said,\n",
      "\"Hey, I've got some good news.\"\n",
      "And I said,\n",
      "\"What's up?\"\n",
      "He said,\n",
      "\"I applied to go back to college now.\"\n",
      "And I said,\n",
      "\"Yeah!\"\n",
      "He said,\n",
      "\"I got accepted.\"\n",
      "And I said,\n",
      "\"Oh, cool.\"\n",
      "And he said,\n",
      "\"We're gonna do it.\"\n",
      "And I said,\n",
      "\"Wait, what?\"\n",
      "He said,\n",
      "\"We're gonna do it.\"\n",
      "So we had a meeting at a bar,\n",
      "which is kind of funny because my parents own the bar.\n",
      "And they were like,\n",
      "\"We're gonna start a new college.\"\n",
      "It's going to be a community college.\n",
      "And they're gonna name it after me\n",
      "because I'm like the only one who likes me.\n",
      "I'm gonna name it after myself.\n",
      "It's gonna be called Michael's Community College.\n",
      "That's what it's gonna be called.\n",
      "They're gonna build a new dormitory on campus,\n",
      "which I think is gonna be me.\n",
      "So I'm like,\n",
      "\"Hey, I'm in.\"\n",
      "And he says,\n",
      "\"I'm gonna be the president of it.\"\n",
      "And I'm like,\n",
      "\"Are you sure?\"\n",
      "He's like,\n",
      "\"Yes.\"\n",
      "And he goes like,\n",
      "\"I wanna make it great again.\"\n",
      "I'm like,\n",
      "\"Are you sure?\"\n",
      "He says,\n",
      "\"You know, when they used to go on those cruises,\n",
      "they're gonna go on a cruise and it's gonna be great again.\"\n",
      "I said,\n",
      "\"Are you sure?\"\n",
      "And he was like,\n",
      "\"Oh, yeah, totally.\"\n",
      "He said,\n",
      "\"I just wanna make it great again.\"\n",
      "And I said,\n",
      "\"Well, okay then.\"\n",
      "So he goes and applies,\n",
      "and they send him out here,\n",
      "and he's like,\n",
      "\"Okay, I don't know if you guys noticed,\n",
      "but there's this giant white male sitting in front of us\n",
      "that's been talking to himself all night long,\n",
      "and he doesn't know how to stop talking to himself,\n",
      "so we should probably stop listening to him.\"\n",
      "And then I realized my dad wasn't really listening.\n",
      "And so he got his job,\n",
      "so he started going.\n",
      "And I would go with him.\n",
      "And it was interesting because I found out\n",
      "that my dad had been a student.\n",
      "And I had no idea that he had gone to school.\n",
      "I was like,\n",
      "\"Why didn't you tell me this before?\"\n",
      "He was like,\n",
      "\"I did tell you.\"\n",
      "I was like,\n",
      "\"What? No, I didn't.\"\n",
      "So it turns out I did grow up around a college person.\n",
      "I just didn't know I had grown up around a college person.\n",
      "It's like a dog being a dog,\n",
      "or a horse being a horse.\n",
      "I didn't know I was a college kid.\n",
      "I always thought I was a regular kid.\n",
      "And I thought my parents were normal too.\n",
      "But I realize now that they weren't.\n",
      "And it was pretty wild.\n",
      "He told me a story once where they\n",
      "had a student that was just super aggressive,\n",
      "and he would just keep hitting people,\n",
      "and they kept trying to get him out.\n",
      "And the police showed up,\n",
      "and they put him in jail.\n",
      "And my dad was like,\n",
      "\"How's that working out for you guys?\n",
      "Like how is that going to help you?\n",
      "How are you gonna get rid of that guy?\"\n",
      "And he's like,\n",
      "\"We're gonna wait for him to get out,\n",
      "and he'll be gone forever.\"\n",
      "And he's like,\n",
      "\"I guess we just need to wait for him to get out.\"\n",
      "And he's like,\n",
      "\"I think he gets out in three months.\"\n",
      "And he's like,\n",
      "\"He's not coming back.\"\n",
      "He's like,\n",
      "\"It's gonna work out fine.\"\n",
      "And that's what my dad does now.\n",
      "That's how my dad lives his life now.\n",
      "He believes things without actually believing them.\n",
      "He believes them.\n",
      "He just doesn't have proof yet,\n",
      "so he's just gonna wait until he gets proof,\n",
      "and then he will believe in the thing.\n",
      "And that's how he lived his whole entire life.\n",
      "That's how he grew up,\n",
      "that's how he grew up in school,\n",
      "and that's how he grew up as an adult.\n",
      "And that's how he grew up as a business owner,\n",
      "and that's how he grows up now.\n",
      "And I don't think I can blame him,\n",
      "because my dad's very young.\n",
      "He's only five years old.\n",
      "That's very young for a human.\n",
      "And so I don't blame him for having a lot of these issues\n",
      "with growing up,\n",
      "and I think that his life is just like my life,\n",
      "which has been so interesting,\n",
      "it's been really great to have grown up with someone who's so young.\n",
      "And he's gotten a lot of good advice out of the fact that he's so young.\n",
      "The only advice I ever get now is advice\n",
      "from when I was younger,\n",
      "and it's a lot better than advice from when I was older.\n",
      "And I think a lot of my dad's advice\n",
      "is based off of when I was younger,\n",
      "and I think that's great.\n",
      "And it's nice because he knows what he didn't know when he was younger,\n",
      "and I don't know what I didn't know when I was younger,\n",
      "and so I feel like maybe he's gonna be able to give me more advice\n",
      "than I can give myself.\n",
      "So yeah, I love having him in my life.\n",
      "[laughter]\n",
      "Yeah.\n",
      "There's a lot of reasons why I love being his son,\n",
      "but one of them is that I can just talk to him about\n",
      "whatever I want to talk to him about,\n",
      "and he won't judge me for it.\n",
      "I can just come out of my house\n",
      "and be like,\n",
      "\"Hey, dad.\"\n",
      "And I can just tell him anything.\n",
      "And he just listens,\n",
      "and he thinks it's all amazing.\n",
      "He loves everything I do.\n",
      "And I love everything he does.\n",
      "He has a lot of great habits that I love to see\n",
      "that I wish I could copy,\n",
      "like he just goes for walks in the park,\n",
      "and I wish I could walk in the park,\n",
      "and he just goes for walks and he just talks to himself,\n",
      "and he just walks through his thoughts,\n",
      "and I wish I could do that more often.\n",
      "I feel like I'm stuck inside my head,\n",
      "and sometimes I want to just go for a walk.\n",
      "And it would be nice to be able to talk to myself in public,\n",
      "but I think that that's something I have to work on.\n",
      "I feel like that's just something I have to learn.\n",
      "Because I think that just sounds a little crazy,\n",
      "to be like,\n",
      "\"Hey, dad,\n",
      "I'm thinking about this,\n",
      "and then I thought about this.\"\n",
      "I feel like people might not react well to that,\n",
      "but I want to try it someday.\n",
      "I want to try walking in the park\n",
      "and just talking to myself\n",
      "and going on long walks like my dad does.\n",
      "He has a lot of good habits that I admire,\n",
      "but he also has a lot of bad habits,\n",
      "too.\n",
      "And I find that the bad habits\n",
      "that my dad has,\n",
      "he's not aware of it.\n",
      "And sometimes I find it interesting\n",
      "that the way I look at things,\n",
      "the bad habits I've gotten from my dad,\n",
      "because I think that my dad\n",
      "has so many bad habits.\n",
      "And I find that I just sort of look at my dad\n",
      "the way I look at other people,\n",
      "the way that I would look at people I don't know,\n",
      "and I would look at them,\n",
      "and I would be like,\n",
      "\"That person is doing a bad habit.\n",
      "Oh, yeah, he's smoking cigarettes right there,\n",
      "and she's eating ice cream.\n",
      "Oh, yeah, he's eating junk food right there.\"\n",
      "That's how I look at my dad.\n",
      "And I find that I'm not quite as judgmental\n",
      "of my own bad habits,\n",
      "because I think I understand where they come from,\n",
      "because I think that my dad taught me\n",
      "a lot of them.\n",
      "And I can recognize them,\n",
      "and I can see what I do.\n",
      "And I can see how they are bad.\n",
      "But I also can see how they aren't bad\n",
      "because I understand where they come from,\n",
      "and I think that's what makes them bad,\n",
      "and I think that's what makes me be able to forgive them,\n",
      "and I think that makes me be able to change them.\n",
      "And I think that's kind of nice,\n",
      "because I think that I can sort of recognize my bad habits\n",
      "and be like,\n",
      "\"Oh, yeah, that was dad,\n",
      "and I'm gonna get rid of that thing.\"\n",
      "And I can do that more quickly\n",
      "because I can recognize the source\n",
      "and then I can figure out how to get rid of it.\n",
      "And I think that helps a lot,\n",
      "because if I didn't know\n",
      "that these bad habits came from my dad,\n",
      "I might think I had to change myself,\n",
      "when what I really needed to do was change my dad.\n",
      "Which I don't really think is possible,\n",
      "but I do think that it might be possible,\n",
      "if I changed the way I interacted\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "ft = PeftModel.from_pretrained(base, \"qwen25-standup-sft/adapter\")\n",
    "ft.eval()\n",
    "\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\": SYSTEM_PROMPT},\n",
    "    {\"role\":\"user\",\"content\": \"Write a 15-minute stand-up set about university student. Include 3 callbacks. Use [laughter] tags.\"}\n",
    "]\n",
    "\n",
    "if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "else:\n",
    "    prompt = f\"<|system|>\\n{SYSTEM_PROMPT}\\n<|user|>\\n{messages[-1]['content']}\\n<|assistant|>\\n\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(ft.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = ft.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=2000,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc6623-6f88-46e9-8709-addfb6abb838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
